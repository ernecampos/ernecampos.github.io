
# Be aware that even a small syntax error here can lead to failures in output.
#

sidebar:
  position: right # position of the sidebar : left or right
  about: True # set to False or comment line if you want to remove the "how to use?" in the sidebar
  education: True # set to False if you want education in main section instead of in sidebar

  # Profile information
  name: Ernesto Campos
  tagline: Adjunct professor at ITESM
  avatar: profile.jpg #place a 100x100 picture inside /assets/images/ folder and provide the name of the file below

  # Sidebar links
  email: e.campos@tec.mx
  scholar: scholar.google.com/citations?user=X0ekpucAAAAJ&hl=en
  linkedin: ernecampos
  github: ernecampos
  pdf: https://ernecampos.github.io/assets/pdfs/CV_Ernesto.pdf # Add a PDF link here if you want to include a PDF custom version in your resume.

  languages:
    title: Languages
    info:
    - idiom: Spanish
      level: Native

    - idiom: English
      level: Professional

    - idiom: Russian
      level: Intermediate

  # interests:
  #   title: Interests
  #   info:
  #   - item: Climbing
  #     link:


  #   - item: Snowboarding
  #     link:


  #   - item: Cooking
  #     link:


career-profile:
  title: About
  summary: |
    I am a researcher in quantum computing and an adjunct professor of computer science at the Monterrey Institute of Technology and Higher Education (ITESM), where I am leading the initiative to establish a new school of quantum computing in Mexico. My journey in this field has previously taken me to the École de technologie supérieure as a visiting faculty member, collaborating with Professor Jacob Biamonte’s group, and to the Skolkovo Institute of Science and Technology (Skoltech) in Moscow—an institute co-founded by MIT—where I served as a research scientist.
    
    At the heart of my research lies a single, overarching question: <b> What are the fundamental trainability limits of quantum algorithmic models, and how can we overcome them to develop more effective training strategies? <b>
    
    My contributions to the field of variational quantum algorithms have been a continuous effort to illuminate this question. In 2021, my work identified several critical phenomena that collectively reshaped our understanding of how and why training can fail. We discovered abrupt training transitions in variational quantum compilation, revealing that the cost landscapes of shallow circuits can be deceptively full of trivial solutions, often demanding a substantially greater depth to reach acceptable results—a finding that aligned with concurrent research showing that such landscapes are, in general, riddled with local minima [cite]. That same year, we characterized a phenomenon we termed "training saturation," demonstrating that circuits optimized in a layerwise fashion hit a fundamental ceiling, unable to improve beyond a certain depth. This insight has directly informed the development of subsequent training strategies, enabling researchers to avoid this pitfall in layerwise-inspired approaches \cite{}. Furthermore, we proved the existence of parameter concentrations, showing that optimal parameter values tend to cluster for different instances of the same problem. This effect, which I had observed empirically in my earlier work on quantum walks, provided a crucial, practical clue for what was to come.
    
    Collectively, these insights were not just a catalog of obstacles; they laid the groundwork for a more constructive line of inquiry. In 2022, we proposed a direct response to these challenges by advocating for the use of quantum hardware-native operations as the building blocks for variational algorithms. Our work showed that these hardware-native, problem-tailored ansatzes not only outperformed standard QAOA for certain combinatorial optimization problems but, crucially, subsequent research has confirmed they also exhibit significantly better training behavior than their generic counterparts.
    
    Most recently, in 2024, by proving the depth scaling of unstructured search via the Quantum Approximate Optimization Algorithm (QAOA), we demonstrated that its oracle query complexity is nearly identical to Grover's algorithm. This result formally confirmed a quantum speedup, corroborating findings from researchers at NASA.
    
    Together, these findings have become foundational to what are now considered some of the most promising families of training strategies: warm start initialization and dynamically built ansatz circuits. More importantly, they have shed light on a greater variety of training limitations—phenomena often overshadowed by the infamous barren plateaus. The integration of this work into the broader literature, where it is cited in multiple reviews [cite] as a key consideration for problem-specific training design, underscores its impact.
    
    Despite this progress, the frontier of quantum machine learning still holds many open questions. We need a deeper, more fundamental understanding of training dynamics that moves beyond simplified models. The most pressing question remains: Are there trainable, non-dequantizable quantum models that can offer a demonstrable advantage over classical machine learning for certain relevant problems? Answering this question continues to drive my research forward.

experiences:
  title: Main collaborators
      - Jacob Biamonte (ÉTS Montreal)
      - Salvador Venegas (ITESM)
      - Daniil Ravinovich (Skoltech, RQC)
      - Akshay Vishwanathan (Nanyang Technological University)
      - Soumik Adhikary (Centre for Quantum Technologies)
      - Alexey Uvarov (Univeristy of Toronto)
      
education:
  title: Education
  info:
  - degree: PhD in Computer Science
    university: Skol;kovo Institute of Science and Technology
    time: 2020 - 2024
    details: |
      Describe your study here lorem ipsum dolor sit amet, consectetuer
      adipiscing elit. Aenean commodo ligula eget dolor. Aenean massa. Cum
      sociis natoque penatibus et magnis dis parturient montes, nascetur
      ridiculus mus. Donec quam felis, ultricies nec, pellentesque eu,
      pretium quis, sem.
        - Bullet point
        - Bullet point
  - degree: MSc in Computer Science
    university: Monterrey Institute of Technology and Higher Education
    time: 2018 - 2019
    details: |
      Describe your study here lorem ipsum dolor sit amet, consectetuer
      adipiscing elit. Aenean commodo ligula eget dolor. Aenean massa. Cum
      sociis natoque penatibus et magnis dis parturient montes, nascetur
      ridiculus mus. Donec quam felis, ultricies nec, pellentesque eu,
      pretium quis, sem.
        - Bullet point
        - Bullet point
  - degree: BSc in Engineering Physics
    university: Monterrey Institute of Technology and Higher Education
    time: 2012 - 2017
    details: |
      Describe your study here lorem ipsum dolor sit amet, consectetuer
      adipiscing elit. Aenean commodo ligula eget dolor. Aenean massa. Cum
      sociis natoque penatibus et magnis dis parturient montes, nascetur
      ridiculus mus. Donec quam felis, ultricies nec, pellentesque eu,
      pretium quis, sem.
        - Bullet point
        - Bullet point


publications:
  title: Publications
  papers:
    
  - title: "Depth scaling of unstructured search via quantum approximate optimization"
    link: "https://doi.org/10.1103/PhysRevA.110.012428"
    authors: <b>E. Campos</b>, D. Rabinovich, A. Uvarov
    conference: Physical Review A <b>110.1</b> (2024), 012428
      
  - title: "On the gate-error robustness of variational quantum algorithms"
    link: "https://doi.org/10.1103/PhysRevA.109.042426"
    authors: D. Rabinovich, <b>E. Campos</b>, S. Adhikary, E. Pankovets, D. Vinichenko, J. Biamonte
    conference: Physical Review A <b>109.4</b> (2024), 042426
      
  - title: "Circuit depth scaling for quantum approximate optimization"
    link: "https://doi.org/10.1103/PhysRevA.106.042438"
    authors: V. Akshay, H. Philathong, <b>E. Campos</b>, D. Rabinovich, I. Zacharov, X. Zhang, J. Biamonte
    conference: Physical Review A <b>106</b>, 024238 (2022)
      
  - title: "Ion native variational ansatz for quantum approximate optimization"
    link: "https://doi.org/10.1103/PhysRevA.106.032418"
    authors: D. Rabinovich, S. Adhikary, <b>E. Campos</b>, V. Akshay, E. Anikin, R. Sengupta, O. Lakhmanskaya, K. Lakhmanskiy, J. Biamonte
    conference: Physical Review A <b>106</b>, 032418 (2022)
      
  - title: "Progress towards analytically optimal angles in quantum approximate optimisation"
    link: "https://doi.org/10.3390/math10152601"
    authors: D. Rabinovich, R. Sengupta, <b>E. Campos</b>, V. Akshay, J. Biamonte
    conference: Mathematics <b>10</b>(15), 2601 (2022)
      
  - title: "Training saturation in layerwise quantum approximate optimisation"
    link: "https://doi.org/10.1103/PhysRevA.104.L030401"
    authors: <b>E. Campos</b>, D. Rabinovich, V. Akshay, J. Biamonte
    conference: (Letter) Physical Review A <b>104</b>, L030401 (2021)
      
  - title: "Quantum walks as algorithmic resources to solve hard k-SAT instances"
    link: "https://doi.org/10.1038/s41598-021-95801-1"
    authors: <b>E. Campos</b>, S. Venegas, M. Lanzagorta
    conference: Scientific Reports <b>11</b>, 16845 (2021)
      
  - title: "Parameter concentrations in quantum approximate optimization"
    link: "https://doi.org/10.1103/PhysRevA.104.L010401"
    authors: V. Akshay, D. Rabinovich, <b>E. Campos</b>, J. Biamonte
    conference: (Letter) Physical Review A <b>104</b>, L010401 (2021)
      
  - title: "Abrupt transitions in variational quantum circuit training"
    link: "https://doi.org/10.1103/PhysRevA.103.032607"
    authors: <b>E. Campos</b>, A. Nasrallah, J. Biamonte
    conference: Physical Review A <b>103</b>, 032607 (2021)
      
  - title: "Heuristic ansatz design for trainable ion-native digital-analog quantum circuits"
    link: "https://arxiv.org/abs/2505.15898"
    authors: G. Paradezhenko, D. Rabinovich, <b>E. Campos</b>, K. Lakhmanskiy
    conference: arXiv:2505.15898 (May 2025)


footer: >
  Designed with <i class="fas fa-heart"></i> by <a href="http://themes.3rdwavemedia.com" target="_blank" rel="nofollow">Xiaoying Riley</a>
